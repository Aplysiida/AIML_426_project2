DE 

DE Design: 

Differentiable evolution was chosen for this problem since DEs were originally designed for optimizing continuous values which this problem has. Because of the abundance of local minima found in the Griewanks function, exploration is encouraged for this problem, thus the scheme DE/best/2 is selected with best used to balance the exploration with exploitation.  

Greedy selector was used for DE to encourage exploitation since DE has a high rate of exploration from mutation generating an entirely different vector unlike in GA, which has similarities to DE in structure, where mutation usually changes only one element in the individual and crossover is based only on individuals already present in the population. 

DE Parameters: 

The hyperparameters to decide for DE are the scaling factor and crossover rate. The scaling factor defines how much the mutation vector will mutate and the crossover rate states the probability of a feature in the individual will be swapped with the mutated result. Increasing either of these rates will increase the exploration done by DE. 

Since DE has high exploration, setting both the scaling factor and crossover rate to low values is recommended to prevent DE from never reaching an optimum within a reasonable number of iterations. It was found through testing various values for the hyperparameters that setting the crossover rate and the scaling factor to 0.1 gives good results for both functions. 

It was found that the DE never converges when maximum number of iterations is set to 1000. Increasing the population to more than 50 individuals did not greatly improve the performance but increasing the maximum number of iterations did, thus the maximum number of iterations was set to 3000. While the DE algorithm still does not converge, it was determined that the number of iterations was a good balance between finding the optimal value and the time taken to calculate. 

Results and Discussion 

When comparing EP and DE, it is noted that DE achieves a much better performance for fitness average for all problems. Where Griewanks is x18 smaller, Rosenbrocks D=20 is x110 smaller and Rosenbrocks D=50 is x56 smaller. This can be because DE has a greater value set for the maximum number of iterations. Thus, DE is able to be closer to convergence compared to EP. 

The reason why DE has a greater maximum number of iterations than EP is because EP takes longer to calculate. The average time taken for EP to calculate one generation was 0.0163 seconds, while for DE it was 0.00328 seconds, roughly x10 quicker than EP. Meaning that DE can complete more iterations in a shorter amount of time. The reason why DE is quicker is because EP uses expensive functions such as Numpy’s exponent function, the implementation is also not optimized as well leading to redundant multiple uses of these expensive functions. DE only uses multiplication and addition operators and thus avoids using expensive functions in each generation. 

For the Rosenbrocks problems, the standard deviation measured for both algorithms was very high, being even bigger than the average calculated. These values for standard deviation, combined with the observation that all problems’ average number of iterations are the maximum number of iterations, indicates that both algorithms have not reached convergence. Which means they were still at a point where exploration is focused more than exploitation. 
For the Griewank problem the standard deviation is smaller than the average calculated. Which indicates that this problem was closer to convergence compared to the others. This is because the problem has a smaller solution space to explore compared to the other two problems. For the Rosenbrock functions reaching the global minimum is difficult because the difference between the global minimum and its surrounding values is small. Thus the algorithm can easily miss it and avoid converging properly. 

Conclusion 

More iterations are needed to reach the true optimal output for these algorithms from reaching convergence. This is easier to achieve with DE due to its quicker execution time per generation compared to EP. The better performance of DE can be the result of DE being originally designed for optimizing continuous problems, while EP were first designed for evolving finite state machines and was modified to work with continuous problems. This has led to using expensive calculations that become very time-consuming when added up through the generations. 

Part 2 

EDA Design: 

Individual representation: 

EDAs are used to solve combinatorial/binary optimization problems; for the knapsack problem to be compatible with the EDA algorithm the representation of the solution should be a binary vector. The entire bit vector represents all the items which can be picked up, where for each bit 1 represents the item is currently being selected and 0 represents the item being ignored. Which item is selected is determined by the position of the bit in the vector. This representation can represent any possible combination of selected items and thus is a good choice for this problem. 

Fitness Function: 

The goal for the problem is to find a combination of items that have the highest total value while also satisfying the weight constraint. For optimizing the fitness to the maximum total value, the sum of values in the item combination is calculated. To handle the weight constraint in the problem a penalty coefficient called alpha is implemented into the fitness to heavily discourage combinations that violate the weight constraint by lowering the fitness. The formula implemented is $insert function here$. 

EDA Algorithm: 

There are two algorithms to select from for this problem: UMDA and PBIL. UMBA calculates the probability vector as the mean vector of the current generation’s population’s individuals while PBIL's probability is influenced only by the best and worst individuals in the population. PBIL focuses more on exploitation compared to UMDA since PBIL ignores individuals in the population that UMDA considers. But exploration can be implemented into PBIL using the mutation operator, something that UMDA does not have. This balance between exploitation and exploration means that PBIL was chosen for this problem. 

PBIL Design: 

Talk about if chose to use mutation function for probability 

The mutation operator in PBIL is used to encourage exploration in the solution space but it introduces more hyperparameters, which are the mutation rate and mutation shift parameters, to tune and an extra calculation in the probability vector evaluation. Even with these issues, for this problem the mutation operator is implemented to help understand all parts of the PBIL algorithm. The formula used to mutate the probability vector is $insert mutation formula here$. 

What stopping criteria is used 

PBIL stops when the generation either reaches the maximum number of iterations or reaches convergence. Convergence is defined by 20 generations where the changes in the best average fitness between the generations is too small to be notable. Convergence is implemented to improve execution time of problems that were likely to be solved early. The maximum number of iterations stopping criteria is used to prevent PBIL from running forever if convergence is never achieved in the problem. 

How a new knapsack solution is generated 

New solutions are generated using the probability vector which stores the probability that each bit’s value will be 1.  PBIL iterates through each element of the probability vector and generates a bit based on the probability at the current vector position and stores that bit in the same position in the individual. The result will be a bit vector used to represent the knapsack selected item combination. 

How probability vector evolves 

The probability vector evolves using three stages. The first stage iterates through the best individuals in the population and changes the probability vector using $insert formula here$. The second stage iterates through the worst individuals in the population and changes the probability vector using the same formula from the first stage but with $-learning rate$ instead. The third stage is the mutation operator which mutates the values in the probability vector using the formula $insert formula here$, the parameters $mutation rate$ and $mutation shift$ are defined by the user. 

EDA Parameters: 

The hyperparameters used by PBIL are    

population size, maximum iterations, mutation rate, mutation shift, number of best individuals, number of worst individuals, maximum probability value allowed, minimum probability value allowed and learning rate. 

The initial values set for population size, mutation rate and mutation shift are the values defined in the original PBIL paper.  

For the first dataset, the solution space of valid solutions is small, so exploitation is focused on for this dataset to find the best solution quickly. Thus, the range of probability values allowed is set to [0.02, 0.98] to encourage retaining the information from the previous populations in the probability vector. The number of best and worst solutions to look at is set to 2 for each to once again encourage a greedier approach to finding the optimal solution.  

For the second dataset, the solution space is bigger, so more exploration is encouraged, the probability range is decreased to [0.05, 0.95] and the number of best and worst solutions is increased to 5 to increase the amount the probability vector and populations can change through the generations. 

The third dataset has a very strict weight constraint and many items to choose from, thus the penalty coefficient is increased to 30 to heavily discourage constraint violating solutions since they are very easy to generate for this dataset. Since the dataset has more items to pick from compared to the previous two datasets, the population is doubled to help explore the solution space in fewer generations.  When tuning the hyperparameters for the dataset, it was found that having too high exploration would cause the model to often produce solutions that violate the weight constraint, so exploitation was encouraged through decreasing the number of best and worst individuals to select for influencing the probability vector. 

Results and Discussion 

The average fitness of the best is calculated by using the best individuals in the population, this uses the same best individuals which are used for influencing the probability vector. 

10_269 

All five seeds have reached the same optimal value of 295. All seeds reach convergence well before the maximum number of iterations showcasing that PBIL quickly finds the optimal solution before satisfying the convergence stopping criteria. 

All the curves rise rapidly from very low fitness to the optimal fitness value. Seeds 1429 and 1667 do explore the solution space a bit with some generations achieving lower accuracy compared to its previous generation. 1667 even goes beyond the optimal value, indicating it explored solutions that violate the weight constraint, before decreasing to optimal fitness. This shows that PBIL can avoid getting caught in solutions that might seem ideal but violate the weight constraint. 

23_10000 

All seeds except for one reach the optimal fitness, but even the suboptimal solution was close to the optimal fitness. All seeds converge much earlier than at the maximum number of iterations, including the suboptimal value, which indicates the suboptimal solution got caught in a local maximum. To avoid such situations from occurring, more exploration should be implemented through tuning the hyperparameters by increasing the mutation rate and shift, increasing the range of possible probability values or through increasing the number of best and worst individuals to influence the probability vector. 

For all seeds, there was a lot of exploration in the early generations where the average fitness of the best individuals varied a lot for each generation. The extreme fitness changes found in the very beginning can be caused by the probability vector being initialized as a sequence of 0.5. The population generated from this probability vector were not aware of the weight constraints and lots of individuals, while having a high value, violated the weight constraint. Over time this exploration decreased and PBIL focused more on exploitation as the difference between each generation decreased over time until PBIL reaches convergence and stops. 

100_995 

None of the seeds reached optimal fitness with the mean being smaller than the optimal value of 1514. The standard deviation being high and the low number of iterations shows that all solutions reached convergence from within a local optimum instead of reaching the maximum number of iterations.  

The shape of the convergence curves supports this observation of premature convergence since the curve is very smooth, which shows that the PBIL does very little exploration and focuses too much on exploitation. More exploration will have produced a less smooth curve as the model explores solutions that do not always improve on the previous solution. There is evidence in the curve shape that exploration happens since there are bumps of slightly worse fitness in the curve. 

Conclusion 

PBIL works well for the first two datasets where it converges to the optimal solution in less than 100 iterations. These datasets were also where hyperparameters tuned for preference for  exploitation over exploration works well.  

For the third dataset PBIL struggles with finding solutions that do not violate the weight constraint despite setting the penalty coefficient to a very high value. This is because of the nature of the probability vector and how the solutions in each generation can continue violating the weight constraint because they are generated based on the probability vector and you can get “unlucky” with the probabilities. This could be avoided by increasing the range of possible values for the probabilities to allow probabilities to be 1.0 and 0.0, but this can lead to getting caught in local optima such as what happened in this implementation. One possible way to avoid generating solutions that are heavier than the weight constraint is by only selecting individuals that do not violate the weight constraint for the best individuals for the population. This can lead to issues with exploration through ignoring solutions that only slightly violate the weight constraint but provide a very big value increase. 

Another issue with using PBIL is that there are a lot of hyperparameters to tune to suit the problem. Tuning these parameters for the third dataset was very time consuming due to the large solution space PBIL explores, which means finding the global optimal value would require a lot of generations before reaching convergence. While there likely is a combination of values for the hyperparameters that does allow the PBIL to find the optimal value before reaching the maximum number of iterations, testing to find this combination is very time consuming to do. 

Part 3: 

GP Design: 

Function and Terminal Sets: 

The CCGP tree program needs to be able to produce all values within the $\mathbb{R}$ domain. The functions used by the program are the arithmetic and trigonometric functions. The arithmetic functions are addition, negation, multiplication and protected division. Protected division is used to prevent errors from occurring when dividing by 0. The trigonometric functions sin and cos are used since $f_2(x)$ uses the sin function and a more accurate representation of this function can be achieved through using cos and sin directly instead of evaluating only using arithmetic.  

Fitness Function and Evaluation 

The complete solution is the combination of the $ f_1(x) $ species and $f_2(x)$ species. The individuals from these species are combined by: 

#insert pseudocode here of if statement and functions 

The fitness function used to evaluate the fitness between the complete solution and the problem function $f(x)$ is MSE. It is used by defining a range of points to test on and calculating the MSE between the output of the points through the complete solution function and output of the x points in $f(x)$. Defined for this problem are 30 points with equal spacing in-between, all defined in the range $[-6.0, 15.0]$. The reason why there are more values in $x \ge 0$ than in $x < 0$ is to avoid the values generated by $f(x \ge 0)$ being too small compared to $f(x < 0)$ such that the complete solution can ignore the variation generated by $sin(x)$. 

MSE is chosen over RMSE since the RMSE requires an extra square root calculation which can be time consuming and the GP only cares about minimizing the different which is easily measured through MSE. 

CCGP Parameters 

The genetic operators used for CCGP are the same ones used for Project 1. These were: one point crossover, uniform mutation and the ramp-half-and-half method for generating trees. These were chosen because the problem to solve for this part is the same problem in Project 1, so the same operators can be used for each species. 

The population size of each of the species is defined to be 200, the crossover and mutation rates are set to 0.95 and 0.15 respectively. These values were set to encourage exploration, since tournament selection can lead to duplicate parents being used in the offspring population when the same individual wins multiple tournaments, which can decrease variety in the individuals.  

For the stopping criteria, the maximum number of iterations is set to 100. 
With the genetic operators used it is possible that the tree does not change much between generations, thus increasing the number of generations done for the algorithm can lead to little exploration. 
Therefore population is more important for GP and the population size is greater than the maximum number of iterations. Since the maximum number of iterations is small enough to avoid very long execution times, checking for convergence is not done for this part. 

Results and Discussion 

Structure: 

There are two trees for each seed, one representing the function used to model $f_1(x > 0)$ and another used to model the function $f_1(x \le 0)$. These two trees are combined into the final solution through an if statement checking for $(x > 0)$. 

The structure of the trees varies between the seeds, with depths ranging from 1 to the maximum depth of 17. For seed 17, the $f_1(x > 0)$ tree is the second biggest tree with depth of 16, yet seeds 47 and 162 model this function with a very simple tree representing sin(x). Indicating that the initial tree never grew new nodes since it was a good enough model. This shows that the initial generation of the trees can greatly affect the structure of the tree. For $f_1(x \le 0)$ the trees tend to be more complicated ranging in depth from 3 to 17, but seeds 17 and 36 are instances where $f_1(x \le 0)$ was simpler than the other tree. 

Performance: 

All runs achieved a very good performance for $f_1(x > 0)$ where the comparison charts showcase all points being very close to the true points. The performance for $f_2(x \le 0)$ varies more and has more errors. Seeds 17 and 162 estimate the function as a straight line while 35 and 36 estimate a curve that is slightly off and for seed 47 there is a less smooth estimation. The seeds that achieves the smallest error rate are seeds 47 and 36 with their MSE values below 2 while all other seeds’ MSE are closer to 6.  

The difference in error between the two sides of the function is caused by $f_2(x \le 0)$ being more complicated for the tree to interpretate compared to $f_1(x > 0)$. $f_1(x > 0)$ only has the functions: division, addition and sin. $f_2(x \le 0)$ has four times as many additions (3 counts as multiple additions since $3=1+1+1$) and multiplications occurring. This also explains the greater tree depth for the GP trees trying to mimic this function. Greater accuracy for this side can be achieved by creating more primitives for the function and terminal sets, such as terminals representing more numbers than just 1 and a function primitive for representing squaring numbers. Having specific function primitives is why the other side achieved much greater accuracy since sin was defined in the primitive set. 

The convergence curves measures how quickly the CCGP algorithm approaches convergence, this varies for different seeds with seeds 17 and 162 reaching convergence quickly and then not changing much for the remaining generations. While seed 47 takes almost the maximum number of iterations to reach convergence. Seeds 17, 36 and 162 have areas in the curve where generations do not change for a while, indicating that the CCGP algorithm does not prioritize exploration enough as it gets stuck in local minima. Increasing the crossover and mutation rates or modifying the tournament selection can improve exploration. 

Conclusion 

The trees generated for this problem were much simpler than the trees generated for Project 1 for the same problem, with only one tree in any of the runs reaching the maximum depth. This is because CCGP separates the problem into two problems that are trained through the two species defined. The two problems are combined using an if statement. If statements are difficult to model using only arithmetic functions, so it is possible to greatly simplify the task through splitting which means that CCGP was able to achieve very good results in less than 100 iterations.  

 This shows that CCGP is very useful for problems which can be split up into simpler subproblems, where it can achieve greater performance in fewer generations when compared to traditional GP. 

Part 4  

FLGP Best GP Tree:  

FEI_1  

The best tree for FEI\_1 is small with a depth of only 2 and involves applying a local scale-invariant feature transform (SIFT) onto a rectangular region on the image. The tree does not branch off into multiple branches(this is ignoring the terminal nodes) and uses only one part of the image described with the leaves for the rectangular region primitive.   

Chart of tree  

Image of selected region  

The FEI\_1 FLGP program ignores expected features that you would use to identify the expression such as mouth and eyes. Instead focusing on part of the right eyebrow and temple of the head. These regions do not have any visible differences between the neutral expression and smile expression. Local SIFT defines features using difference of Gaussian, an approximation of Laplacian of Gaussian which is used to detect regions that differ in value. These regions are likely the lighter value region of the temple and the darker value region of the eyebrow. So, SIFT could be determining the expression through the overall position and rotation of the head determined by the temple and eyebrow region where smiling could lead to the person facing the camera in a slightly different, but notable angle.  

FEI_2  

The FLGP tree for FEI/_2 is larger than the tree generated for FEI/_1, with a depth of 4 and, excluding the leaf nodes, 5 branches. The tree considers both local and global features, for the global features the histogram of oriented gradients(HOG), differential item functioning(DIF) and SIFT are used. These descriptors use the gradients in the image to determine features has areas in the image with abrupt changes. While for the local features the SIFT and local binary patterns(LBP) are used. LBP's feature vectors are histograms of each of the cells/regions in the image counting the frequency of pixels with a greater value than the central pixel of the cell. So LBP are used to describe information about the pixels' neighbours.  

Chart of GP Tree  

Image of selected region  

What is interesting is that FEI\_2 FLGP considers a similar local region that the FEI_1 FLGP looks at (green box). Another local region is used which looks at the right ear and cheekbone of the face (red box). This indicates that FEI\_2 is following a similar approach to FEI\_1 of considering the orientation of the head as important for determining the expression of the face through the local regions. Unlike FEI\_1, this FLGP uses more feature extraction functions to extract more data from the face and since FEI\_2 uses global features as well, the FLGP tree also considers the other parts of the face that FEI\_1's FLGP does not consider. 

Classifier Design and Evaluation:  

For testing the performance of the FLGP algorithm, some classifiers were trained and tested on the feature vector data generated from the FLGP's best performing program. This is done separately for the FEI\_1 and FEI\_2 datasets. Since the classifiers are only being used to test the performance, performance of the classifier itself does not matter, but execution time can matter for measuring if FLGP can be used to help evalulate the performance of the FLGP outputs quickly.  

The classifiers chosen were Decision Trees, Naïve Bayes and K Nearest Neighbours (KNN). These were chosen to explore classifiers that excel at different issues e.g., discrete vs continuous problems, assumption of independence, etc. The metrics measured are the accuracy of the classifier based on the separate test dataset and the time taken to train the classifier through the training dataset. These metrics will showcase if the FLGP output can generate a good quality dataset that classifiers can use effectively.  

Classifier Results Discussion:  

FEI\_1 achieves bad performance for decision trees and KNN but achieves good performance for naïve Bayes. For the training time, KNN was the fastest and decision trees were the slowest, being 1.58 times slower than KNN. The fast-training times is caused by the dataset being small with only 150 instances and 129 features for each instance. Achieving a good performance on naïve Bayes indicates that the assumption of the features being independent is valid. The reason why KNN and decision trees had bad performance in FEI\_1 could be because of the large number of features present in the feature vectors. The number of features present is almost equal to the number of instances, which can lead to overfitting to the training data and thus the classifier models will struggle with generalizing.  

FEI\_2 achieves better performance compared to FEI\_1 for all classifiers. Once again naïve Bayes achieves the best performance but is 0.02/% worse than FEI\_1’s naïve Bayes. Both decision trees and KNN achieve better performance compared to FEI\_1 with decision trees having the biggest improvement of 38/%. This improvement is interesting because the number of features present in the feature vectors have increased from 129 to 570 for this dataset. Suggesting that overfitting is not as much of an issue as initially thought. Reasons why there is a greater improvement could be that the FLGP for FEI\_2 is able to identify more useful features to be used by the classifier. For the training time naïve Bayes is 2.3 times slower compared to FEI\_1 naïve Bayes, while decision trees and KNN achieve small reductions in training time. This shows that despite the increase of dimensions to handle by the classifier, training time performance is not heavily affected for most classifiers, though the effect the number of dimensions present in the feature vectors has on the training time could become noticeable in larger datasets with more instances.  

Conclusion:  

It was expected that the FLGP's local regions would be at facial features that people use to determine expressions such as around the mouth, eyes and eyebrows. The actual local regions used were the rightmost side of the face, while this contained part of an eyebrow, no other expected features were used. The reasons why FLGP did not choose the mouth and eyes could be that there was too much variation between smiles from different people at those features, e.g., showing or not showing teeth, mouth shape, eye shape, position of eyes and mouth in image, etc. There is still variation in different people in the local regions selected through the hairline covering some parts of the image and the overall shape and size of peoples' heads, but these regions must still be consistent enough to the FLGP for this region to be a reliable source of information. 

It is noted that naive Bayes exceeded the other two classifiers in accuracy performance for both FEI\_1 and FEI\_2. The reason why this classifier is successful could be from the number of dimensions that need to be handled and that naïve Bayes can better handle large dimensionality. To improve the performance of the other two classifiers, decreasing the number of features in the feature vectors through dimension reduction and feature selection techniques could be done. 

Part 5 

CartPole-v1 Problem: 

The CartPole-v1 problem is about balancing a pole hinged onto a cart for as long as possible. The cart and pole exist in a space where only the left and right direction matters. Thus, the cart can only move along one axis and the pole can only rotate around one axis orthogonal to the cart’s translation axis.  

An episode in this simulation is composed of a sequence of time step where each time step has a state. The program will update the state’s variables and will apply an action from the action space to the state in each time step. The maximum number of time steps for one episode is 500, where when exceeded the episode will terminate. 

A state in this problem is represented through four variables: cart position $x$, cart velocity $dx$, pole angle $\theta$ and pole angular velocity $d\theta$. These variables can be either continuous or discrete depending on the accuracy the user desires for this problem. There is a range of valid values for the position and angle variables where, when outside this range, causes the episode to terminate early. For the cart position the range of valid values is $[-2.4, 2.4]$ and for the pole angle the range in degrees is $[-12, 12]$. The initial state for this problem used at the beginning of the episode is a random value in the range [-0.05, 0.05] for all four-state variables. 

The action space for this problem is composed of two actions: push the cart to the left and push the cart to the right. This pushing applies a fixed force to the cart and changes the cart’s velocity and through the cart’s velocity, changes the pole’s angular force. 

The criteria of successful learning for this problem is maximizing the number of timesteps taken by an episode while avoiding exceeding the range values defined for cart position and pole angle. The total reward value is initialized at 0 at the initial stage of the episode. For each successful time step where no termination criteria were satisfied, the total reward value is incremented by 1.  
