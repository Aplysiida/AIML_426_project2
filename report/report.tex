\documentclass{article}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{subcaption}

\title{AIML426 Project 2 Report}
\date{}

\begin{document}
	\maketitle
	
\section*{Part 1: Evolutionary Programming and Differential Evolution Algorithms}
\subsection*{EP Design}
\subsection*{DE Design}
\subsection*{Results}
\subsubsection*{Evolutionary Programming}
\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Average & Standard Deviation & Average Number of Iterations \\
		\hline
		Rosenbrock D=20 & 54128.0185 & 39159.198 & 2000.0 \\
		\hline
		Griewanks D=20 & 0.941 & 0.0615 & 2000.0 \\
		\hline
		Rosenbrock D=50 & 52689978.820 & 80423786.869 & 2000.0 \\
		\hline
	\end{tabular}
\end{center}

\subsubsection*{Differential Evolution}
\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Average & Standard Deviation & Average Number of Iterations \\
		\hline
		Rosenbrock D=20 & 491.359 & 1380.648 & 3000.0 \\
		\hline
		Griewanks D=20 & 0.0523 & 0.0399 & 3000.0 \\
		\hline
		Rosenbrock D=50 & 927869.272 & 1194001.554 & 3000.0 \\
		\hline
	\end{tabular}
\end{center}

\section*{Part 2: Estimation of Distribution Algorithm}
\subsection*{EDA Design}
\subsubsection*{Individual Representation}
EDAs are used to solve combinatorial/binary optimization problems; for the knapsack problem to be compatible with the EDA algorithm the representation of the solution should be a binary vector. The entire bit vector represents all the items which can be picked up, where for each bit 1 represents the item is currently being selected and 0 represents the item being ignored. Which item is selected is determined by the position of the bit in the vector. This representation can represent any possible combination of selected items and thus is a good choice for this problem. \par
\subsubsection*{Fitness Function}
The goal for the problem is to find a combination of items that have the highest total value while also satisfying the weight constraint. For optimizing the fitness to the maximum total value, the sum of values in the item combination is calculated. To handle the weight constraint in the problem a penalty coefficient called $\alpha$ is implemented into the fitness to heavily discourage combinations that violate the weight constraint by lowering the fitness. \par 
\noindent The formula implemented is 
\begin{center}
$max(0, \sum_{i=1}^{M}v_i - \alpha *max(0,\sum_{i=1}^{M}w_i)-max weight)$. 
\end{center}
Where $M=$ the number of chosen items in the solution, $v=$ value and $w=$ weight.
\subsubsection*{EDA Algorithm}
There are two algorithms to select from for this problem: UMDA and PBIL. UMBA calculates the probability vector as the mean vector of the current generation’s population’s individuals while PBIL's probability is influenced only by the best and worst individuals in the population. PBIL focuses more on exploitation compared to UMDA since PBIL ignores individuals in the population that UMDA considers. But exploration can be implemented into PBIL using the mutation operator, something that UMDA does not have. This balance between exploitation and exploration means that PBIL was chosen for this problem. \par
\subsubsection*{PBIL Design}
The mutation operator in PBIL is used to encourage exploration in the solution space but it introduces more hyperparameters, which are the mutation rate and mutation shift parameters, to tune and an extra calculation in the probability vector evaluation. Even with these issues, for this problem the mutation operator is implemented to help understand all parts of the PBIL algorithm. The psuedocode of the mutation function is shown below: \par
\begin{algorithm}
	\caption{PBIL Probability Mutation}
	\begin{algorithmic}
		\For{$i \gets length(p) $}
			\If{$random[0,1] < mut\_rate$}
				\State{$p_i \gets p_i * (1.0-mut\_shift) + random(0.0 or 1.0) * mut\_shift$}
			\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}
\noindent PBIL stops when the generation either reaches the maximum number of iterations or reaches convergence. Convergence is defined by 20 generations where the changes in the best average fitness between the generations is too small to be notable. Convergence is implemented to improve execution time of problems that were likely to be solved early. The maximum number of iterations stopping criteria is used to prevent PBIL from running forever if convergence is never achieved in the problem. \par
\noindent New solutions are generated using the probability vector which stores the probability that each bit’s value will be 1.  PBIL iterates through each element of the probability vector and generates a bit based on the probability at the current vector position and stores that bit in the same position in the individual. The result will be a bit vector used to represent the knapsack selected item combination. \par
\noindent The probability vector evolves using three stages. The first stage iterates through the best individuals in the population and changes the probability vector using: \par 
\begin{algorithm}
	\caption{PBIL Best Individuals Influence}
	\begin{algorithmic}
		\For{$i \gets best\_N $}
			\State{$p \gets p + \eta * (ind_i - p)$}
		\EndFor
	\end{algorithmic}
\end{algorithm}
The second stage iterates through the worst individuals in the population and changes the probability vector using the same formula from the first stage but using $-\eta$ for $\eta$ instead. The third stage is the mutation operator which mutates the values in the probability vector using the formula with the parameters $mut\_rate$ and $mut\_shift$ are defined by the user. \par
\subsubsection*{EDA Parameters}
The hyperparameters used by PBIL are 
\begin{itemize}
	\item Population Size
	\item Maximum Iterations
	\item Mutation Rate
	\item Mutation Shift
	\item Number of Best Individuals
	\item Number of Worst Individuals
	\item Maximum Probability
	\item Minimum Probability
	\item Learning Rate
\end{itemize}
The initial values set for population size, mutation rate and mutation shift are the values defined in the original PBIL paper \cite{baluja1994population}. These values are:
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	Population Size & 100 \\
	\hline
	Mutation Rate & 0.02 \\
	\hline
	Mutation Shift & 0.05 \\
	\hline
\end{tabular}
\end{center}
\noindent For the first dataset, the solution space of valid solutions is small, so exploitation is focused on for this dataset to find the best solution quickly. Thus, the range of probability values allowed is set to [0.02, 0.98] to encourage retaining the information from the previous populations in the probability vector. The number of best and worst solutions to look at is set to 2 for each to once again encourage a greedier approach to finding the optimal solution. \par
\noindent For the second dataset, the solution space is bigger, so more exploration is encouraged, the probability range is decreased to [0.05, 0.95] and the number of best and worst solutions is increased to 5 to increase the amount the probability vector and populations can change through the generations. \par
\noindent add third dataset parameters \par

\subsection*{Results and Discussion}
The average fitness of the best is calculated by using the best individuals in the population, this uses the same best individuals which are used for influencing the probability vector. \par 
\subsection*{10\_269 Dataset}
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	Average Fitness of Best Mean & Average Fitness of Best Standard Deviation \\
	\hline
	295.0 & 0.0 \\
	\hline
\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Seed & Number of iterations \\
		\hline
		1562 & 24 \\
		\hline
		1574 & 25 \\
		\hline
		1429 & 27 \\
		\hline
		1667 & 27 \\
		\hline
		1647 & 26 \\
		\hline
	\end{tabular}
\end{center}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{knapsack_10_269.png}
	\caption{Convergence Curve}
\end{figure}

\subsection*{23\_10000 Dataset}
\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Average Fitness of Best Average & Average Fitness of Best Standard Deviation \\
		\hline
		9766.6 & 0.7999.. \\
		\hline
	\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Seed & Number of iterations \\
		\hline
		1562 & 49 \\
		\hline
		1574 & 48 \\
		\hline
		1429 & 90 \\
		\hline
		1667 & 52 \\
		\hline
		1647 & 50 \\
		\hline
	\end{tabular}
\end{center}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{knapsack_23_10000.png}
	\caption{Convergence Curve}
\end{figure}

\subsection*{100\_995 Dataset}
\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Average Fitness of Best Average & Average Fitness of Best Standard Deviation \\
		\hline
		1435.4 & 40.9712 \\
		\hline
	\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{|c|c|}
		\hline
		Seed & Number of iterations \\
		\hline
		1562 & 65 \\
		\hline
		1574 & 58 \\
		\hline
		1429 & 50 \\
		\hline
		1667 & 55 \\
		\hline
		1647 & 64 \\
		\hline
	\end{tabular}
\end{center}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{knapsack_100_995.png}
	\caption{Convergence Curve}
\end{figure}

\subsection*{Conclusion}

\section*{Part 3: Cooperative Co-evolution Genetic Programming}
\subsection*{CCGP Design}
\subsubsection*{Function and Terminal Sets}
\subsubsection*{Fitness Function and Evaluation}
\subsubsection*{CCGP Parameters}
\subsection*{Results and Discussion}

\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		Seed & Best Fitness & Best Depth $f_1(x > 0)$ & Best Depth $f_2(x \le 0)$ \\
		\hline
		17 & 5.934 & 16 & 3 \\
		\hline
		35 & 5.975 & 1 & 3 \\
		\hline
		36 & 1.753 & 6 & 5 \\
		\hline
		47 & 1.589 & 1 & 17 \\
		\hline
		162 & 5.975 & 1 & 3 \\
		\hline
	\end{tabular}
\end{center}

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ccgp_chart_17.png}
	\end{subfigure}
\begin{subfigure}[b]{\linewidth}
	\includegraphics[width=\linewidth]{ccgp_chart_35.png}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
	\includegraphics[width=\linewidth]{ccgp_chart_36.png}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
	\includegraphics[width=\linewidth]{ccgp_chart_47.png}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
	\includegraphics[width=\linewidth]{ccgp_chart_162.png}
\end{subfigure}
	\caption{Comparison of true values(blue points) and CCGP generated function values(orange points).}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ccgp_curve_17.png}
	\end{subfigure}
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ccgp_curve_35.png}
	\end{subfigure}
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ccgp_curve_36.png}
	\end{subfigure}
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ccgp_curve_47.png}
	\end{subfigure}
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ccgp_curve_162.png}
	\end{subfigure}
	\caption{Convergence curves for each seed.}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_17_1.png}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_17_2.png}
		\caption{$f_1(x > 0)$ tree(left) and $f_2(x \le 0)$ tree(right) for seed 17}
	\end{subfigure}
\end{figure}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_35_1.png}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_35_2.png}
		\caption{$f_1(x > 0)$ tree(left) and $f_2(x \le 0)$ tree(right) for seed 35}
	\end{subfigure}
\end{figure}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_36_1.png}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_36_2.png}
		\caption{$f_1(x > 0)$ tree(left) and $f_2(x \le 0)$ tree(right) for seed 36}
	\end{subfigure}
\end{figure}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=0.1\linewidth]{ccgp_best_tree_47_1.png}
	\end{subfigure}
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=\linewidth]{ccgp_best_tree_47_2.png}
	\end{subfigure}
	\caption{$f_1(x > 0)$ tree(top) and $f_2(x \le 0)$ tree(bottom) for seed 47}
\end{figure}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{\linewidth}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_162_1.png}
		\includegraphics[width=0.5\linewidth]{ccgp_best_tree_162_2.png}
		\caption{$f_1(x > 0)$ tree(left) and $f_2(x \le 0)$ tree(right) for seed 162}
	\end{subfigure}
\end{figure}
	
\subsubsection*{Structure}
\subsubsection*{Perfomance}
	
\subsection*{Conclusion}

\section*{Part 4: Genetic Programming for Image Classification}
\subsection*{Classifier Design and Evaluation}
\subsection*{Classifier Results and Discussion}
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Dataset & Accuracy(\%) & Training Time(seconds) \\
		\hline
		FEI\_1 & 100.0 & 0.00280 \\
		\hline
		FEI\_2 & 0.0 & 0.0 \\
		\hline
	\end{tabular}
\end{center}
\subsection*{Conclusion}

\bibliographystyle{acm}
\bibliography{references}

\end{document}